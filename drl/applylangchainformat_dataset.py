from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from datasets import Dataset
import logging
import json
import re

# setup logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

ds_code = Dataset.load_from_disk("/home/sureshm/ssuresh/code_porting_models/dataset/babeltower")
ds_code = ds_code.select(range(100))

logger.info(f"Dataset: {ds_code}")

template = """
            You are a high performance computing software engineer. I want you to translates the cpp or c++ code to CUDA code. \n 
            Input code given: \n
            {cpp_code} \n
            Please think and reason step by step to achieve code porting task and
            Output should clearly differentiate between the thought process, summary and the code. \n
            Always provide the complete translated CUDA and always place the CUDA code in the section call "CUDA code" \n
            Start the output with '<think>\n' \n
            """

messages = [ {
    "role": "user", "content": template.format(cpp_code=ds_code["code"][8])
    }]

cpp_code = ds_code["code"][8]
prompt = ChatPromptTemplate.from_template(template)
formatted_prompt_template = prompt.format(
    cpp_code=cpp_code
)

model_name = "deepseek-r1:32b"
llm = ChatOllama(model="deepseek-r1:32b", temperature=0.6, num_predict=-1)
response = llm.invoke(formatted_prompt_template)


def parsemessage(ai_message: AIMessage) -> str:
    """Parse the AI message."""
    message = ai_message.content.swapcase()
    match = re.search(r"<think>(.*?)</think>", message, re.DOTALL | re.IGNORECASE)
    code_match = re.search(r"```(.*?)```", message, re.DOTALL)
    kernel_match = re.search(r"__GLOBAL__\s+VOID\s+\w+\(.*?\)\s*{.*?}", message, re.DOTALL | re.IGNORECASE)

    if match:
        think_content = match.group(1).strip()
    else:
        logger.warning("No <think> tag found in the message.")
        think_content = ""

    if code_match:
        summary_content = re.sub(r"<think>.*?</think>", "", message, flags=re.DOTALL | re.IGNORECASE).strip()
        cuda_code = code_match.group(1).strip()
    else:
        logger.warning("No <code> tag found in the message.")
        summary_content = message.strip()
        cuda_code = ""
    
    if kernel_match:
        kernel_code = kernel_match.group(0).strip()
    else:
        logger.warning("No kernel code found in the message.")
        kernel_code = ""

    
    return think_content, summary_content, cuda_code, kernel_code


think_content, summary_content, cuda_code, kernel_code = parsemessage(AIMessage(content=response.content))

def constructOPjson(**kwargs):
    return {
        "synthetic_data": "Synthetic data generated by the model",
        "model_name": model_name,
        "cpp_code": cpp_code,
        "think_content": think_content,
        "summary_content": summary_content,
        "cuda_code": cuda_code,
        "kernel_code": kernel_code,
    }

res_dict = constructOPjson(model_name=model_name, 
                            cpp_code=ds_code["code"][10], 
                            think_content=think_content, 
                            summary_content=summary_content, 
                            cuda_code=cuda_code, 
                            kernel_code=kernel_code)

with open("output.json", "w") as f:
    json.dump(res_dict, f, indent=4)
    logger.info("Output saved to output.json")
    f.close()

