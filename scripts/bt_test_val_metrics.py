from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from datasets import Dataset
import logging
import json
import re
from tqdm import tqdm
import os

# setup logger
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

ds_code_test = Dataset.load_from_disk("/home/sureshm/ssuresh/code_porting_models/dataset/babeltower_test_val")
#ds_code = ds_code.select(range(100))

#logger.info(f"Dataset: {ds_code_test}")

infer = False
#infer = True
eva = True
#eva = False

#model_name = "deepseek-r1:32b"
#model_name = "deepseek-r1:1.5b"
#model_name = "deepseek-r1:7b"
#model_name = "deepseek-r1:8b"
#model_name = "deepseek-r1:14b"
model_name = "deepseek-r1:70b"
#model_name = "deepseek-coder-v2:16b"
#model_name = "deepseek-coder-v2:236b"

llm = ChatOllama(model=model_name, temperature=0.6, num_predict=-1)

if infer:
    template = """
                You are a high performance computing software engineer. I want you to translates the cpp or c++ code to CUDA code. \n 
                Input code given: \n
                {cpp_code} \n
                Please think and reason step by step to achieve code porting task and
                Output should clearly differentiate between the thought process, summary and the code. \n
                Always provide the complete translated CUDA and always place the CUDA code in the section call "CUDA code" \n
                Use the same variable names as in the input code. \n
                Start the output with '<think>\n' \n
                """

    def append_to_json(file_path, new_data):
        # Check if the file exists
        if os.path.exists(file_path):
            # Load existing data
            with open(file_path, "r") as f:
                existing_data = json.load(f)
        else:
            # If the file doesn't exist, start with an empty list
            existing_data = []

        # Append the new data
        existing_data.append(new_data)

        # Write the updated data back to the file
        with open(file_path, "w") as f:
            json.dump(existing_data, f, indent=4)
            logger.info(f"Data appended to {file_path}")


    def parsemessage(ai_message: AIMessage) -> str:
        """Parse the AI message."""
        message = ai_message.content.swapcase()
        match = re.search(r"<think>(.*?)</think>", message, re.DOTALL | re.IGNORECASE)
        code_match = re.search(r"```(.*?)```", message, re.DOTALL)
        kernel_match = re.search(r"__GLOBAL__\s+VOID\s+\w+\(.*?\)\s*{.*?}", message, re.DOTALL | re.IGNORECASE)

        if match:
            think_content = match.group(1).strip()
        else:
            logger.warning("No <think> tag found in the message.")
            think_content = ""

        if code_match:
            summary_content = re.sub(r"<think>.*?</think>", "", message, flags=re.DOTALL | re.IGNORECASE).strip()
            cuda_code = code_match.group(1).strip()
        else:
            logger.warning("No <code> tag found in the message.")
            summary_content = message.strip()
            cuda_code = ""
        
        if kernel_match:
            kernel_code = kernel_match.group(0).strip()
        else:
            logger.warning("No kernel code found in the message.")
            kernel_code = ""

        
        return think_content, summary_content, cuda_code, kernel_code

    def constructOPjson(**kwargs):
        return {
            "synthetic_data": "Synthetic data generated by the model",
            "model_name": model_name,
            "cpp_code": cpp_code,
            "think_content": think_content,
            "summary_content": summary_content,
            "cuda_code": cuda_code,
            "kernel_code": kernel_code,
            "reference": ref
        }
    
    results = []
    predictions = []
    references = []
    ############## Inference all ##############
    for idx, cpp_code in tqdm(enumerate(ds_code_test["cpp"])):
        logger.info(f"Processing sample {idx + 1}/{len(ds_code_test)}")

        prompt = ChatPromptTemplate.from_template(template)
        formatted_prompt_template = prompt.format(cpp_code=cpp_code)

        response = llm.invoke(formatted_prompt_template)
        think_content, summary_content, cuda_code, kernel_code = parsemessage(AIMessage(content=response.content))

        ref = ds_code_test["cuda"][idx]
        res_dict = constructOPjson(model_name=model_name, 
                                    cpp_code=cpp_code, 
                                    think_content=think_content, 
                                    summary_content=summary_content, 
                                    cuda_code=cuda_code, 
                                    kernel_code=kernel_code,
                                    ref=ref
                                    )
        results.append(res_dict)
        predictions.append(kernel_code)
        references.append(ds_code_test["cuda"][idx])
        output_file = model_name + "tes_val_inference.json"
        append_to_json(output_file, res_dict)

# Read the JSON file
# with open("tes_val_inference.json", "r") as f:
#     data = json.load(f)
#     print(data)

# Convert json to dataset
# ds_code_test = Dataset.from_dict(data)
# print(ds_code_test)

if eva:
    # Calculate Model Metrics on BT test val dataset
    # set the path to BASE_DIR
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    import src.train_models.evaluate as evaluate

    evaluator = evaluate.Evaluate()
    metric_results = evaluator.eval("deepseek-r1:70btes_val_inference.json")
    #metric_results = evaluator.eval("tes_val_inference.json")
    logger.info(f"Metric Results: {metric_results}")